# Environment variables: follow .env.example file
volumes:
  antigravity-manager-data:

x-common-healthcheck: &common-healthcheck
  interval: 30s # Perform health check every 30 seconds
  timeout: 10s # Health check command times out after 10 seconds
  retries: 3 # Retry up to 3 times if health check fails

x-common-deploy: &common-deploy
  replicas: 1
  update_config:
    parallelism: 1
    delay: 10s
    order: start-first
    failure_action: rollback
  rollback_config:
    parallelism: 1
    delay: 10s
    order: stop-first

x-antigravity-manager: &antigravity-manager
  image: lbjlaq/antigravity-manager:v4.1.10
  environment:
    - API_KEY=${ANTIGRAVITY_MANAGER_API_KEY}
    - WEB_PASSWORD=${ANTIGRAVITY_MANAGER_WEB_PASSWORD}
    - ABV_MAX_BODY_SIZE=104857600
  networks:
    - internal
    - public
    - monitoring
  healthcheck:
    <<: *common-healthcheck
    test:
      [
        "CMD-SHELL",
        "python3 -c \"import urllib.request; urllib.request.urlopen('http://127.0.0.1:8045/health')\"",
      ]
    start_period: 30s

services:
  antigravity-manager:
    <<: *antigravity-manager
    volumes:
      - antigravity-manager-data:/root/.antigravity_tools
    deploy:
      <<: *common-deploy
      labels:
        - traefik.enable=true
        - traefik.http.routers.antigravity-manager.rule=Host(`${ANTIGRAVITY_MANAGER_HOST}`)
        - traefik.http.routers.antigravity-manager.entrypoints=websecure
        - traefik.http.routers.antigravity-manager.tls.certresolver=${LETSENCRYPT_RESOLVER:-le}
        - traefik.http.services.antigravity-manager.loadbalancer.server.port=8045
        - traefik.http.routers.antigravity-manager.service=antigravity-manager

  litellm:
    image: ghcr.io/berriai/litellm-database:main-v1.81.3.rc.2
    command: ["--config", "/app/config.yaml", "--num_workers", "4"]
    environment:
      DATABASE_URL: "postgresql://${DB_USER:-app}:${DB_PASSWORD:-app_db_PW@2018}@db:5432/${DB_NAME:-app}"
      STORE_MODEL_IN_DB: "True" # allows adding models to proxy via UI
      LITELLM_MASTER_KEY: ${LITELLM_MASTER_KEY:-litellm_master_key}
      LITELLM_SALT_KEY: ${LITELLM_SALT_KEY:-litellm_salt_key}
      LITELLM_MODE: PRODUCTION
      LITELLM_LOG: ERROR
      JSON_LOGS: "True"
      USE_PRISMA_MIGRATE: "True"
      SEPARATE_HEALTH_APP: "1"
      SEPARATE_HEALTH_PORT: "4001"
      LITELLM_DROP_PARAMS: "True"
    configs:
      - source: litellm-config-yaml
        target: /app/config.yaml
    healthcheck: # Defines the health check configuration for the container
      <<: *common-healthcheck
      test:
        - CMD-SHELL
        - python3 -c "import urllib.request; urllib.request.urlopen('http://localhost:4001/health/liveliness')" # Command to execute for health check
      start_period: 40s # Wait 40 seconds after container start before beginning health checks
    networks:
      - internal
      - public
      - monitoring
    deploy:
      <<: *common-deploy
      labels:
        - traefik.enable=true
        - traefik.http.routers.litellm.rule=Host(`${LITELLM_HOST}`)
        - traefik.http.routers.litellm.entrypoints=websecure
        - traefik.http.routers.litellm.tls.certresolver=${LETSENCRYPT_RESOLVER:-le}
        - traefik.http.services.litellm.loadbalancer.server.port=4000
        - traefik.http.routers.litellm.service=litellm

  # Database cleanup job - prunes old spend logs to prevent disk exhaustion
  db-cleanup:
    image: postgres:16-alpine
    environment:
      PGHOST: db
      PGUSER: ${DB_USER:-app}
      PGPASSWORD: ${DB_PASSWORD:-app_db_PW@2018}
      PGDATABASE: ${DB_NAME:-app}
      RETENTION_DAYS: ${DB_CLEANUP_RETENTION_DAYS:-365}
    command:
      - /bin/sh
      - -c
      - |
        echo "Starting database cleanup job..."
        echo "Deleting spend logs older than $${RETENTION_DAYS} days..."
        psql -c "DELETE FROM \"LiteLLM_SpendLogs\" WHERE \"startTime\" < NOW() - INTERVAL '$${RETENTION_DAYS} days';"
        echo "Running VACUUM ANALYZE..."
        psql -c "VACUUM ANALYZE \"LiteLLM_SpendLogs\";"
        echo "Cleanup completed at $$(date)"
    networks:
      - internal
    deploy:
      mode: replicated
      replicas: 0
      labels:
        - swarm.cronjob.enable=true
        - swarm.cronjob.schedule=0 3 * * 0
        - swarm.cronjob.skip-running=true
      restart_policy:
        condition: none

configs:
  litellm-config-yaml:
    name: ${LITELLM_CONFIG_NAME:-llmproxy_litellm-config-yaml}
    external: true

networks:
  internal:
    name: ${PUBLIC_NETWORK:-llmproxy}
    external: true
  public:
    name: ${PUBLIC_NETWORK:-public}
    external: true
  monitoring:
    name: ${MONITORING_NETWORK:-monitoring}
    external: true
